\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{float}
\usepackage{hyperref}

\title{CV701 Assignment 4 -- Task 1 Report}
\author{Mashrafi Monon}
\date{\today}

\begin{document}
\maketitle

\section{Introduction}
Task~1 of CV701 Assignment~4 requires creating a deep learning model that detects facial keypoints using only the supplied dataset and then using the predicted landmarks to classify the displayed emotion. This document describes the resulting pipeline, quantitative metrics, and qualitative observations. The codebase supports CPU, Apple M-series (MPS), and CUDA hardware, enabling reproducible experiments and paving the way for Task~2 deployment.

\section{Methodology}
\subsection{Data Pipeline}
The canonical CSV files (\texttt{training\_frames\_keypoints.csv} and \texttt{test\_frames\_keypoints.csv}) list relative image paths and 136 numbers corresponding to 68 $(x, y)$ coordinates. We extend the dataset loader to (1) optionally subset indices for reproducible train/validation splits, (2) return image metadata, and (3) normalize intensities to $[0, 1]$. Each training example passes through:
\begin{itemize}
    \item \textbf{Rescale} the RGB image to $224\times224$ while scaling keypoints.
    \item \textbf{Random horizontal flip} with probability $0.5$ for augmentation.
    \item \textbf{Keypoint normalization} to $[-1, 1]$ using the resized width/height.
    \item \textbf{ToTensor} + ImageNet mean/standard deviation so inputs align with ResNet expectations.
\end{itemize}
We reserve 10\% of the training split for validation and use eight dataloader workers to stream batches efficiently.

\subsection{Model}
The default network starts from a ResNet-18 backbone pre-trained on ImageNet. We remove the classifier head and append a regression block (flatten $\rightarrow$ Linear(512) $\rightarrow$ ReLU $\rightarrow$ Dropout(0.3) $\rightarrow$ Linear(136)). Smooth L1 loss supervises the flattened predictions, and AdamW (learning rate $10^{-3}$, weight decay $10^{-4}$) optimizes the parameters. A cosine annealing scheduler gradually lowers the learning rate across up to 100 epochs (extended schedule for the final submission), and gradient norms are clipped at 5 to prevent instability. We also implemented an optional heatmap head (ResNet backbone plus a deconvolutional head that outputs Gaussian heatmaps) for future experimentation, but the final submitted results use the direct-regression version because it converged faster and yielded better accuracy on the provided dataset.

\subsection{Emotion Heuristic}
We denormalize the predicted landmarks into pixel space and compute inter-ocular distance $s = \lVert \mathbf{p}_{45} - \mathbf{p}_{36} \rVert_2$. Mouth geometry ratios are then
\begin{align*}
    w &= \frac{p_{54}^x - p_{48}^x}{s}, \\
    h &= \frac{p_{57}^y - p_{51}^y}{s}, \\
    c &= \frac{\tfrac{1}{2}(p_{48}^y + p_{54}^y) - p_{62}^y}{s}.
\end{align*}
If $c < -0.015$ and $w > 0.7$, we declare a \textbf{positive} expression. If $c > 0.02$ or $h > 0.32$, we classify it as \textbf{negative}. Otherwise the expression is considered \textbf{neutral}. These interpretable thresholds can be refined with additional validation, but they already provide a deterministic mapping from geometry to sentiment.

\section{Experiments}
\subsection{Training Configuration}
The submitted model corresponds to \texttt{artifacts/task1\_resnet18}: batch size 64, learning rate $10^{-3}$, dropout 0.3, cosine annealing, and $100$ epochs on a single NVIDIA GPU (WandB logging enabled). A ResNet-34 variant was also tested, but despite the added capacity it underperformed (NME $\approx 0.36$), so we kept the ResNet-18 configuration for the final report. All experiments relied solely on the provided training split plus the augmentations described above.

\subsection{Validation Metrics}
Figure~\ref{fig:curves} overlays training vs.\ validation loss and the validation error metrics (NME, pixel MAE/RMSE, and PCK@$\{0.05,0.10\}$ using inter-ocular distance). The curves highlight rapid convergence within 20 epochs. The best validation NME ($0.174$) and pixel MAE ($5.38$~px) occur around epoch~34, which is saved as the best checkpoint (minimum validation loss $0.0038$). Tracking PCK and the cumulative-error AUC provides additional context beyond raw loss by revealing how many landmarks fall within tight tolerances.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/task1_curves.png}
    \caption{Training/validation loss and validation error curves for the CUDA experiment.}
    \label{fig:curves}
\end{figure}

\subsection{Test Results}
The selected checkpoint achieves the metrics shown in Table~\ref{tab:test}. Average pixel error is roughly $4.3$~px, NME drops to $0.13$, and $48$\% of landmarks fall within $10$\% of the inter-ocular distance (PCK@0.10) while $16$\% fall within $5$\% (PCK@0.05). The cumulative-error curve up to $0.5$ IoD yields an AUC of $0.74$. The ResNet-34 baseline failed to improve these numbers (PCK@0.10 $\approx 13$\%), so we retained the more accurate ResNet-18 model. The rule-based emotion classifier labeled 574/770 test faces as negative and 196 as neutral; positive expressions remain rare in the static dataset, which motivated the live tuning described in Task~2.

\begin{table}[H]
    \centering
    \begin{tabular}{lcc}
        \toprule
        Metric & Value & Units \\
        \midrule
        Pixel MAE & $4.29$ & px \\
        Pixel RMSE & $6.33$ & px \\
        Mean point error & $6.85$ & px \\
        Normalized mean error (NME) & $0.130$ & -- \\
        PCK@0.05 (IOD) & $16.3$ & \% \\
        PCK@0.10 (IOD) & $48.2$ & \% \\
        AUC@0.5 (IOD) & $0.75$ & -- \\
        Smooth L1 loss & $0.0016$ & -- \\
        \bottomrule
    \end{tabular}
    \caption{Test-set performance for the best validation checkpoint (PCK/AUC normalized by inter-ocular distance).}
    \label{tab:test}
\end{table}

\section{Qualitative Analysis}
Manual inspection of random samples confirms that the regressor tracks eye and nose landmarks tightly on frontal, well-lit faces. Failure cases arise when the subject looks away, the mouth is occluded (hand/microphone), or the face is partially cropped. Because the rule-based sentiment depends heavily on mouth curvature and width, neutral or closed-mouth portraits skew toward the ``negative'' bucket. Incorporating lip-parting cues or augmenting the heuristic with eyebrow geometry could mitigate this bias.

\section{Task 2: Deployment}
\subsection{Setup}
The live demo runs on a MacBook Pro (Apple M2, 16~GB RAM) using the \texttt{src.deploy\_live} CLI. The command below enables emotion overlays, captures a 30~s clip, and logs summary statistics (frames processed, average FPS, emotion counts):
\begin{verbatim}
python -m src.deploy_live \
  --checkpoint artifacts/task1_resnet18/best_model.pt \
  --device mps \
  --backbone resnet18 \
  --show-emotion \
  --record-path demo.mp4 \
  --max-seconds 30 \
  --smooth-momentum 0.0 \
  --emotion-hold 15 \
  --log-summary --log-path demo_summary.json
\end{verbatim}

\subsection{Performance}
The recorded session averages roughly 28--30 FPS on M2 (as reported in \texttt{demo\_summary.json}). Positive labels trigger as soon as the mouth widens and curves upward; pronounced frowns (large mouth height or downward curvature) flip to negative. The optional notebook (\texttt{notebooks/task2\_live\_demo.ipynb}) exposes Start/Stop and Record buttons for GUI control. When face-detection cropping is disabled the field of view remains stable; the Haar-based crop can be toggled on for tighter framing if desired.

\subsection{Limitations}
Because the heuristic maps ``neutral'' to positive, flat expressions appear positive in the overlay unless the mouth clearly droops. Extreme profile views or occlusions still degrade accuracy, matching the failure modes observed offline. Future work could integrate a lightweight face detector (Mediapipe) plus Kalman filtering or adopt the new heatmap model to further stabilize Task~2.

\section{Conclusion and Future Work}
Task~1 now delivers an end-to-end training pipeline, reproducible metrics, a prediction dump, and an interpretable emotion tagger. Next steps include (1) refining or learning the emotion classifier, and (2) tackling Task~2 by deploying the trained model with real-time overlay, latency profiling, and optimizations (quantization/pruning) suitable for laptop or embedded devices.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
